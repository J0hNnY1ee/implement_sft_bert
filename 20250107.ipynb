{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Bert\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert_config, num_labels):\n",
    "        super().__init__()\n",
    "        # 定义BERT模型\n",
    "        self.bert = BertModel(config=bert_config)\n",
    "        # 定义Dropout层\n",
    "        self.dropout = nn.Dropout(p=0.2)  # Dropout概率为0.2\n",
    "        # 定义分类器\n",
    "        self.classifier = nn.Linear(bert_config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # BERT的输出\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        # 取[CLS]位置的pooled output\n",
    "        pooled = bert_output[1]\n",
    "        # 在pooled output上应用Dropout\n",
    "        pooled = self.dropout(pooled)\n",
    "        # 分类\n",
    "        logits = self.classifier(pooled)\n",
    "        # 返回结果\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Bert+BiLSTM，用法与BertClassifier一样，可直接在train里面调用\n",
    "class BertLstmClassifier(nn.Module):\n",
    "    def __init__(self, bert_config, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel(config=bert_config)\n",
    "        self.lstm = nn.LSTM(input_size=bert_config.hidden_size, hidden_size=bert_config.hidden_size, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.classifier = nn.Linear(bert_config.hidden_size*2, num_labels)  # 双向LSTM 需要乘以2\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        out, _ = self.lstm(last_hidden_state)\n",
    "        logits = self.classifier(out[:, -1, :]) # 取最后时刻的输出\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集划分完成：\n",
      "训练集保存在 data/train.json\n",
      "测试集保存在 data/test.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 假设你的 JSON 数据存储在一个文件中\n",
    "input_file = 'data/2016.json'  # 替换为你的 JSON 文件路径\n",
    "output_train_file = 'data/train.json'\n",
    "output_test_file = 'data/test.json'\n",
    "\n",
    "# 读取 JSON 数据\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 确保数据是一个列表\n",
    "if isinstance(data, list):\n",
    "    # 划分数据集\n",
    "    train_data = data[:3109]\n",
    "    test_data = data[3109:]\n",
    "\n",
    "    # 保存训练集和测试集到文件\n",
    "    with open(output_train_file, 'w', encoding='utf-8') as f_train:\n",
    "        json.dump(train_data, f_train, ensure_ascii=False, indent=4)\n",
    "\n",
    "    with open(output_test_file, 'w', encoding='utf-8') as f_test:\n",
    "        json.dump(test_data, f_test, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"数据集划分完成：\\n训练集保存在 {output_train_file}\\n测试集保存在 {output_test_file}\")\n",
    "else:\n",
    "    print(\"JSON 数据不是列表格式，无法划分数据集。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from: test_data.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 405.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: [ 101 6857 3221  671 5063 5080 1606 4638 3152 4995  102    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "Token Type IDs: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Attention Mask: [1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Label ID: 0\n",
      "Label Scores: tensor([0.5000, 0.3333, 0.1667, 0.0000, 0.0000, 0.0000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class SinaNewsDataset(Dataset):\n",
    "    def __init__(self,filename,tokenizer):\n",
    "        \n",
    "        self.labels = [\"moved\",\"angry\",\"funny\",\"sad\",\"novel\",\"shocked\"]\n",
    "        self.labels_id  = list(range(len(self.labels)))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.token_type_ids = []\n",
    "        self.attention_mask = []\n",
    "        self.label_id = []\n",
    "        self.label_scores = []\n",
    "        self.load_data(filename)\n",
    "        \n",
    "    def load_data(self,filename):\n",
    "        print('loading data from:', filename)\n",
    "        with open(filename,'r',encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        for item in tqdm(data):\n",
    "            emotions = {k: v for k, v in item.items() if k in self.labels}\n",
    "            label = max(emotions, key=emotions.get)\n",
    "            text = item['news']\n",
    "            label_id = self.labels.index(label)\n",
    "            token = self.tokenizer(text, add_special_tokens=True, padding='max_length', truncation=True, max_length=512)\n",
    "            self.input_ids.append(np.array(token['input_ids']))\n",
    "            self.token_type_ids.append(np.array(token['token_type_ids']))\n",
    "            self.attention_mask.append(np.array(token['attention_mask']))\n",
    "            self.label_id.append(label_id)\n",
    "            \n",
    "            ## 添加的部分\n",
    "            total = int(item[\"Total\"])\n",
    "            if total > 0:\n",
    "                scores =torch.tensor([emotions[label] for label in self.labels], dtype=torch.float32)\n",
    "                # label_vector = torch.softmax(scores,dim=0)  # 使用 softmax 归一化 \n",
    "                label_vector = (scores / scores.sum())\n",
    "            else:\n",
    "                label_vector = [0.0] * len(self.labels)  # 如果 Total 为 0，所有情感归为 0\n",
    "            self.label_scores.append(label_vector)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.token_type_ids[index], self.attention_mask[index], self.label_id[index],self.label_scores[index],\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "from transformers import BertTokenizer\n",
    "   \n",
    "def test_sina_news_dataset():\n",
    "    # 创建一个测试数据文件\n",
    "    test_data = [\n",
    "        {\n",
    "            \"news\": \"這是一篇簡單的文章\",\n",
    "            \"moved\": 3,\n",
    "            \"angry\": 2,\n",
    "            \"funny\": 1,\n",
    "            \"sad\": 0,\n",
    "            \"novel\": 0,\n",
    "            \"shocked\": 0,\n",
    "            \"Total\": 6\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    test_filename = 'test_data.json'\n",
    "    with open(test_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(test_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "    # 使用 BertTokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('model/bert-base-chinese')\n",
    "\n",
    "    # 加载数据集\n",
    "    dataset = SinaNewsDataset(test_filename, tokenizer)\n",
    "\n",
    "    # 打印第一行\n",
    "    first_item = dataset[0]\n",
    "    print(\"Input IDs:\", first_item[0])\n",
    "    print(\"Token Type IDs:\", first_item[1])\n",
    "    print(\"Attention Mask:\", first_item[2])\n",
    "    print(\"Label ID:\", first_item[3])\n",
    "    print(\"Label Scores:\", first_item[4])\n",
    "\n",
    "# 调用测试函数\n",
    "test_sina_news_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from scipy.stats import pearsonr\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_acc1(pred_labels, labels_score):\n",
    "    \"\"\"\n",
    "    计算 ACC@1\n",
    "    :param pred_labels: 模型预测的标签 (Tensor)\n",
    "    :param labels_score: 真实标签的分数 (Tensor 或列表)\n",
    "    :return: 当前 batch 的 ACC@1\n",
    "    \"\"\"\n",
    "    correct = 0  # 正确计数\n",
    "    batch_size = len(pred_labels)  # 批次大小\n",
    "    label_id_rank = torch.argsort(labels_score, dim=1, descending=True)  # 对情感进行排序\n",
    "    for i in range(batch_size):\n",
    "        pred_label = pred_labels[i].item()\n",
    "        true_rank = label_id_rank[i]\n",
    "        scores = labels_score[i]\n",
    "\n",
    "        # 判断预测是否正确\n",
    "        for rank_idx in range(len(true_rank)):\n",
    "            if scores[true_rank[rank_idx]] == scores[true_rank[0]]:  # 判断是否与第一分数相同\n",
    "                if pred_label == true_rank[rank_idx]:  # 检查预测是否在相同分数的排名中\n",
    "                    correct += 1\n",
    "                    break  # 预测正确，跳出循环\n",
    "            else:\n",
    "                break  # 一旦分数不同，结束判断\n",
    "\n",
    "    return correct / batch_size  # 返回准确率\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(learning_rate = 5e-6):\n",
    "    MODEL_PATH = 'model/bert-base-chinese'\n",
    "    DATA_PATH = 'data/'\n",
    "    batch_size = 16\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    epochs = 20\n",
    "        # Learning Rate不宜太大\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "    train_dataset = SinaNewsDataset(DATA_PATH + 'train.json', tokenizer)\n",
    "    valid_dataset = SinaNewsDataset(DATA_PATH + 'test.json', tokenizer)\n",
    "\n",
    "    bert_config = BertConfig.from_pretrained(MODEL_PATH)\n",
    "    num_labels = len(train_dataset.labels)\n",
    "\n",
    "    model = BertClassifier(bert_config, num_labels).to(device)\n",
    "    model = nn.DataParallel(model)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate,)\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    criterion = nn.KLDivLoss(reduction='batchmean') ## 使用 KLDivLoss 作为损失函数\n",
    "\n",
    "    best_f1 = 0\n",
    "\n",
    "    metrics_history = {\n",
    "        'train_accuracy': [],\n",
    "        'train_pearson': [],\n",
    "        'valid_accuracy': [],\n",
    "        'valid_pearson': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        train_bar = tqdm(train_dataloader, ncols=100)\n",
    "        \n",
    "        train_losses, train_acc,train_pearson,pearson_count  = 0, 0, 0,0\n",
    "\n",
    "        for input_ids, token_type_ids, attention_mask, label_id ,labels_score in train_bar:\n",
    "            model.zero_grad()\n",
    "            train_bar.set_description(f'Epoch {epoch} train')\n",
    "\n",
    "            output = model(\n",
    "                input_ids=input_ids.to(device),\n",
    "                attention_mask=attention_mask.to(device),\n",
    "                token_type_ids=token_type_ids.to(device),\n",
    "            )\n",
    "\n",
    "            # loss = criterion(output, label_id.to(device))\n",
    "            loss = criterion(F.log_softmax(output, dim=1), labels_score.to(device))\n",
    "            train_losses += loss.item()\n",
    "            \n",
    "            pred_labels = torch.argmax(output, dim=1)\n",
    "            acc = calculate_acc1(pred_labels, labels_score)\n",
    "            train_acc += acc\n",
    "            logits = output.clone().detach()\n",
    "            \n",
    "            for i in range(logits.size(0)):\n",
    "                pearson, _ = pearsonr(logits[i].cpu().numpy(), labels_score[i].cpu().detach().numpy())\n",
    "                train_pearson += pearson\n",
    "                pearson_count += 1\n",
    "                \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_bar.set_postfix(loss=loss.item(), acc=acc)\n",
    "            \n",
    "        train_pearson = train_pearson / pearson_count\n",
    "        train_accuracy = train_acc / len(train_dataloader)\n",
    "        metrics_history['train_accuracy'].append(train_accuracy)\n",
    "        metrics_history['train_pearson'].append(train_pearson)\n",
    "        print(f'\\tTrain ACC: {train_accuracy}, Train Pearson: {train_pearson}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        \n",
    "        valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "        valid_bar = tqdm(valid_dataloader, ncols=100)\n",
    "        \n",
    "        valid_losses, valid_acc,valid_pearson, pearson_count,valid_preds, valid_labels = 0, 0,0,0, [], []\n",
    "\n",
    "        for input_ids, token_type_ids, attention_mask, label_id,labels_score in valid_bar:\n",
    "            valid_bar.set_description(f'Epoch {epoch} valid')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(\n",
    "                    input_ids=input_ids.to(device),\n",
    "                    attention_mask=attention_mask.to(device),\n",
    "                    token_type_ids=token_type_ids.to(device),\n",
    "                )\n",
    "\n",
    "            # loss = criterion(output, label_id.to(device))\n",
    "            loss = criterion(F.log_softmax(output, dim=1), labels_score.to(device))\n",
    "\n",
    "            valid_losses += loss.item()\n",
    "\n",
    "            pred_label = torch.argmax(output, dim=1)\n",
    "            # acc = torch.sum(pred_label == label_id.to(device)).item() / len(pred_label)\n",
    "            acc = calculate_acc1(pred_label, labels_score)\n",
    "            valid_acc += acc\n",
    "            logits = output.clone()\n",
    "            for i in range(logits.size(0)):\n",
    "                    pearson, _ = pearsonr(logits[i].cpu().numpy(), labels_score[i].cpu().numpy())\n",
    "                    valid_pearson += pearson\n",
    "                    pearson_count += 1\n",
    "\n",
    "            valid_preds.extend(pred_label.cpu().numpy())\n",
    "            valid_labels.extend(label_id.cpu().numpy())\n",
    "            valid_bar.set_postfix(loss=loss.item(), acc=acc)\n",
    "\n",
    "        \n",
    "        valid_pearson = valid_pearson / pearson_count\n",
    "        valid_accuracy = valid_acc / len(valid_dataloader)\n",
    "        metrics_history['valid_accuracy'].append(valid_accuracy)\n",
    "        metrics_history['valid_pearson'].append(valid_pearson)\n",
    "        print(f'\\tValid ACC: {valid_accuracy}, Valid Pearson: {valid_pearson}')\n",
    "\n",
    "        report = metrics.classification_report(valid_labels, valid_preds, labels=valid_dataset.labels_id, target_names=valid_dataset.labels)\n",
    "        print('* Classification Report:')\n",
    "        print(report)\n",
    "\n",
    "        f1 = metrics.f1_score(valid_labels, valid_preds, labels=valid_dataset.labels_id, average='micro')\n",
    "\n",
    "        if not os.path.exists('models'):\n",
    "            os.makedirs('models')\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save(model.state_dict(), 'model/best_model.pkl')\n",
    "\n",
    "    return metrics_history\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from: data/train.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3109/3109 [00:08<00:00, 351.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from: data/test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2148/2148 [00:06<00:00, 324.81it/s]\n",
      "Epoch 0 train:  52%|███████████████              | 101/195 [00:33<00:31,  2.99it/s, acc=0, loss=nan]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m metrics_historys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr \u001b[38;5;129;01min\u001b[39;00m learning_rate:\n\u001b[0;32m----> 6\u001b[0m     metrics_historys\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 提取每个学习率的最大验证集准确率和皮尔逊系数\u001b[39;00m\n\u001b[1;32m      9\u001b[0m max_valid_accuracy \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mmax\u001b[39m(history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m history \u001b[38;5;129;01min\u001b[39;00m metrics_historys]\n",
      "Cell \u001b[0;32mIn[4], line 106\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(learning_rate)\u001b[0m\n\u001b[1;32m    104\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    105\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 106\u001b[0m     train_bar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, acc\u001b[38;5;241m=\u001b[39macc)\n\u001b[1;32m    108\u001b[0m train_pearson \u001b[38;5;241m=\u001b[39m train_pearson \u001b[38;5;241m/\u001b[39m pearson_count\n\u001b[1;32m    109\u001b[0m train_accuracy \u001b[38;5;241m=\u001b[39m train_acc \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "learning_rate = [5e-7,1e-6,2e-6,5e-6,1e-5,2e-5,5e-5,1e-4,1e-3]\n",
    "metrics_historys = []\n",
    "for lr in learning_rate:\n",
    "    metrics_historys.append(train(lr))\n",
    "    \n",
    "# 提取每个学习率的最大验证集准确率和皮尔逊系数\n",
    "max_valid_accuracy = [max(history['valid_accuracy']) for history in metrics_historys]\n",
    "max_valid_pearson = [max(history['valid_pearson']) for history in metrics_historys]\n",
    "\n",
    "# 保存图表为文件\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# 验证集准确率\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(learning_rate, max_valid_accuracy, marker='o')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate (log scale)')\n",
    "plt.ylabel('Max Validation Accuracy')\n",
    "plt.title('Max Validation Accuracy vs Learning Rate')\n",
    "plt.grid(True)\n",
    "\n",
    "# 验证集皮尔逊系数\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(learning_rate, max_valid_pearson, marker='o', color='orange')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate (log scale)')\n",
    "plt.ylabel('Max Validation Pearson')\n",
    "plt.title('Max Validation Pearson vs Learning Rate')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('learning_rate_vs_metrics.png')\n",
    "print(\"图表已保存为 'learning_rate_vs_metrics.png' 文件。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_metrics(metrics_history):\n",
    "#     epochs = range(1, len(metrics_history['train_accuracy']) + 1)\n",
    "\n",
    "#     plt.figure(figsize=(14, 8))\n",
    "\n",
    "#     # Plot accuracy\n",
    "#     plt.subplot(2, 1, 1)\n",
    "#     plt.plot(epochs, metrics_history['train_accuracy'], label='Train Accuracy', marker='o')\n",
    "#     plt.plot(epochs, metrics_history['valid_accuracy'], label='Valid Accuracy', marker='o')\n",
    "#     plt.title('Training and Validation Accuracy')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.legend()\n",
    "\n",
    "#     # Plot Pearson correlation\n",
    "#     plt.subplot(2, 1, 2)\n",
    "#     plt.plot(epochs, metrics_history['train_pearson'], label='Train Pearson', marker='o')\n",
    "#     plt.plot(epochs, metrics_history['valid_pearson'], label='Valid Pearson', marker='o')\n",
    "#     plt.title('Training and Validation Pearson Correlation')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Pearson Correlation')\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Call the train function and plot metrics\n",
    "# metrics_history = train()\n",
    "# plot_metrics(metrics_history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
