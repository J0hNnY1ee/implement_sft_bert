{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Bert\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert_config, num_labels):\n",
    "        super().__init__()\n",
    "        # 定义BERT模型\n",
    "        self.bert = BertModel(config=bert_config)\n",
    "        # 定义Dropout层\n",
    "        self.dropout = nn.Dropout(p=0.2)  # Dropout概率为0.2\n",
    "        # 定义分类器\n",
    "        self.classifier = nn.Linear(bert_config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # BERT的输出\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        # 取[CLS]位置的pooled output\n",
    "        pooled = bert_output[1]\n",
    "        # 在pooled output上应用Dropout\n",
    "        pooled = self.dropout(pooled)\n",
    "        # 分类\n",
    "        logits = self.classifier(pooled)\n",
    "        # 返回结果\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Bert+BiLSTM，用法与BertClassifier一样，可直接在train里面调用\n",
    "class BertLstmClassifier(nn.Module):\n",
    "    def __init__(self, bert_config, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel(config=bert_config)\n",
    "        self.lstm = nn.LSTM(input_size=bert_config.hidden_size, hidden_size=bert_config.hidden_size, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.classifier = nn.Linear(bert_config.hidden_size*2, num_labels)  # 双向LSTM 需要乘以2\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        out, _ = self.lstm(last_hidden_state)\n",
    "        logits = self.classifier(out[:, -1, :]) # 取最后时刻的输出\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "import csv\n",
    "class SinaNewsDataset(Dataset):\n",
    "    def __init__(self, filename, tokenizer):\n",
    "        # 数据集初始化\n",
    "        self.labels = ['0','1','2','3']\n",
    "        self.labels_id = list(range(4))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.token_type_ids = []\n",
    "        self.attention_mask = []\n",
    "        self.label_id = []\n",
    "        self.load_data(filename)\n",
    "    \n",
    "    def load_data(self, filename):\n",
    "        # 加载数据\n",
    "        print('Loading data from:', filename)\n",
    "        with open(filename, 'r', encoding='utf-8') as rf:\n",
    "            reader = csv.reader(rf)\n",
    "            next(reader)  # 跳过表头，如果没有表头可以注释掉\n",
    "            for line in tqdm(reader, ncols=100):\n",
    "                label_id, text = int(line[0]), line[1]\n",
    "                token = self.tokenizer(\n",
    "                    text,\n",
    "                    add_special_tokens=True,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    max_length=512\n",
    "                )\n",
    "                self.input_ids.append(np.array(token['input_ids']))\n",
    "                self.token_type_ids.append(np.array(token.get('token_type_ids', [])))\n",
    "                self.attention_mask.append(np.array(token['attention_mask']))\n",
    "                self.label_id.append(label_id)\n",
    "\n",
    "        print(f'Data loaded successfully: {len(self.input_ids)} samples.')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.token_type_ids[index], self.attention_mask[index], self.label_id[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    \n",
    "## 取消注释测试\n",
    "# tokenizer = BertTokenizer.from_pretrained('model/bert-base-chinese')\n",
    "# data_loader = SinaNewsDataset(\"data/weibo_4/simplifyweibo_4_moods.csv\",tokenizer)\n",
    "# print(data_loader[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集拆分完成：\n",
      "从原始数据集中随机选取 1/5 的数据。\n",
      "训练集保存至: data/weibo_4/train.csv, 样本数: 57878\n",
      "验证集保存至: data/weibo_4/valid.csv, 样本数: 14470\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "def split_csv(file_path, train_path, val_path):\n",
    "    \"\"\"\n",
    "    先随机取出数据集的1/5，再从中按8:2划分为训练集和验证集。\n",
    "\n",
    "    Args:\n",
    "        file_path (str): 原始 CSV 文件路径。\n",
    "        train_path (str): 训练集文件路径。\n",
    "        val_path (str): 验证集文件路径。\n",
    "    \"\"\"\n",
    "    # 读取数据\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        reader = list(csv.reader(f))\n",
    "        header = reader[0]  # 提取表头\n",
    "        data = reader[1:]   # 提取数据部分\n",
    "\n",
    "    # 随机取出1/5的数据\n",
    "    total_count = len(data)\n",
    "    subset_count = total_count // 5\n",
    "    subset_indices = random.sample(range(total_count), subset_count)\n",
    "    subset_data = [data[i] for i in subset_indices]\n",
    "\n",
    "    # 对这1/5按8:2划分为训练集和验证集\n",
    "    split_index = int(len(subset_data) * 0.8)\n",
    "    random.shuffle(subset_data)  # 再次打乱\n",
    "    train_data = subset_data[:split_index]\n",
    "    val_data = subset_data[split_index:]\n",
    "\n",
    "    # 保存训练集\n",
    "    with open(train_path, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)  # 写入表头\n",
    "        writer.writerows(train_data)  # 写入数据\n",
    "\n",
    "    # 保存验证集\n",
    "    with open(val_path, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)  # 写入表头\n",
    "        writer.writerows(val_data)  # 写入数据\n",
    "\n",
    "    print(f\"数据集拆分完成：\")\n",
    "    print(f\"从原始数据集中随机选取 1/5 的数据。\")\n",
    "    print(f\"训练集保存至: {train_path}, 样本数: {len(train_data)}\")\n",
    "    print(f\"验证集保存至: {val_path}, 样本数: {len(val_data)}\")\n",
    "\n",
    "# 示例用法\n",
    "input_file = 'data/weibo_4/simplifyweibo_4_moods.csv'  # 原始 CSV 文件路径\n",
    "train_file = 'data/weibo_4/train.csv'  # 保存训练集的路径\n",
    "val_file = 'data/weibo_4/valid.csv'  # 保存验证集的路径\n",
    "\n",
    "# 创建随机种子保证复现性（可选）\n",
    "random.seed(42)\n",
    "\n",
    "split_csv(input_file, train_file, val_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: data/weibo_4/train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57878it [00:15, 3856.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully: 57878 samples.\n",
      "Loading data from: data/weibo_4/valid.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14470it [00:03, 3743.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully: 14470 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 train: 100%|██████████████████████| 1809/1809 [20:13<00:00,  1.49it/s, acc=0.636, loss=1.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain ACC: 0.5491701844313784 \tLoss: 1.1699675033934673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 valid: 100%|████████████████████████| 453/453 [01:41<00:00,  4.47it/s, acc=0.667, loss=1.17]\n",
      "/home/lijianhang/anaconda3/envs/LLM/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/lijianhang/anaconda3/envs/LLM/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/lijianhang/anaconda3/envs/LLM/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.101266787541623\n",
      "* Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.98      0.73      8003\n",
      "           1       0.42      0.03      0.06      2113\n",
      "           2       0.35      0.10      0.15      2186\n",
      "           3       0.00      0.00      0.00      2168\n",
      "\n",
      "    accuracy                           0.56     14470\n",
      "   macro avg       0.34      0.28      0.23     14470\n",
      "weighted avg       0.43      0.56      0.43     14470\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 train: 100%|██████████████████████| 1809/1809 [20:15<00:00,  1.49it/s, acc=0.409, loss=1.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain ACC: 0.5597894366551083 \tLoss: 1.1023511673087505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 valid: 100%|████████████████████████| 453/453 [01:41<00:00,  4.47it/s, acc=0.667, loss=0.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.0819044146053576\n",
      "* Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.97      0.74      8003\n",
      "           1       0.32      0.13      0.19      2113\n",
      "           2       0.33      0.02      0.03      2186\n",
      "           3       0.36      0.08      0.13      2168\n",
      "\n",
      "    accuracy                           0.57     14470\n",
      "   macro avg       0.40      0.30      0.27     14470\n",
      "weighted avg       0.48      0.57      0.46     14470\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 train: 100%|█████████████████████| 1809/1809 [20:16<00:00,  1.49it/s, acc=0.636, loss=0.952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain ACC: 0.5719037388813509 \tLoss: 1.0725173981496812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 valid: 100%|██████████████████████████| 453/453 [01:41<00:00,  4.47it/s, acc=0.5, loss=1.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.1050543733779958\n",
      "* Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.99      0.74      8003\n",
      "           1       0.45      0.05      0.09      2113\n",
      "           2       0.39      0.01      0.01      2186\n",
      "           3       0.37      0.13      0.19      2168\n",
      "\n",
      "    accuracy                           0.57     14470\n",
      "   macro avg       0.45      0.29      0.26     14470\n",
      "weighted avg       0.51      0.57      0.45     14470\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 train: 100%|██████████████████████| 1809/1809 [20:14<00:00,  1.49it/s, acc=0.545, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain ACC: 0.579247072717222 \tLoss: 1.053979066836366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 valid: 100%|███████████████████████| 453/453 [01:41<00:00,  4.47it/s, acc=0.667, loss=0.956]\n",
      "/home/lijianhang/anaconda3/envs/LLM/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/lijianhang/anaconda3/envs/LLM/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/lijianhang/anaconda3/envs/LLM/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.0549312774708728\n",
      "* Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.97      0.75      8003\n",
      "           1       0.42      0.13      0.19      2113\n",
      "           2       0.39      0.16      0.23      2186\n",
      "           3       0.00      0.00      0.00      2168\n",
      "\n",
      "    accuracy                           0.58     14470\n",
      "   macro avg       0.35      0.32      0.29     14470\n",
      "weighted avg       0.45      0.58      0.48     14470\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 train: 100%|█████████████████████| 1809/1809 [20:14<00:00,  1.49it/s, acc=0.727, loss=0.721]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain ACC: 0.5862056510377406 \tLoss: 1.035863955980378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 valid: 100%|███████████████████████| 453/453 [01:41<00:00,  4.47it/s, acc=0.833, loss=0.882]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.0578647350633381\n",
      "* Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.94      0.76      8003\n",
      "           1       0.36      0.25      0.30      2113\n",
      "           2       0.36      0.14      0.21      2186\n",
      "           3       0.36      0.05      0.08      2168\n",
      "\n",
      "    accuracy                           0.58     14470\n",
      "   macro avg       0.43      0.35      0.34     14470\n",
      "weighted avg       0.51      0.58      0.51     14470\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 train: 100%|█████████████████████| 1809/1809 [20:15<00:00,  1.49it/s, acc=0.636, loss=0.987]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain ACC: 0.5905950047741093 \tLoss: 1.0250297227638607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 valid: 100%|█████████████████████████| 453/453 [01:41<00:00,  4.46it/s, acc=0.5, loss=0.949]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.055707980596993\n",
      "* Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.96      0.75      8003\n",
      "           1       0.38      0.21      0.27      2113\n",
      "           2       0.39      0.16      0.23      2186\n",
      "           3       0.40      0.02      0.03      2168\n",
      "\n",
      "    accuracy                           0.59     14470\n",
      "   macro avg       0.45      0.34      0.32     14470\n",
      "weighted avg       0.52      0.59      0.49     14470\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 train: 100%|█████████████████████| 1809/1809 [20:15<00:00,  1.49it/s, acc=0.545, loss=0.917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain ACC: 0.5959690185436454 \tLoss: 1.0115163120003021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 valid: 100%|███████████████████████| 453/453 [01:41<00:00,  4.46it/s, acc=0.667, loss=0.742]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.0792742163666562\n",
      "* Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.85      0.75      8003\n",
      "           1       0.30      0.41      0.35      2113\n",
      "           2       0.34      0.12      0.18      2186\n",
      "           3       0.31      0.11      0.16      2168\n",
      "\n",
      "    accuracy                           0.57     14470\n",
      "   macro avg       0.41      0.37      0.36     14470\n",
      "weighted avg       0.52      0.57      0.52     14470\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 train: 100%|█████████████████████| 1809/1809 [20:14<00:00,  1.49it/s, acc=0.636, loss=0.969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain ACC: 0.6014435398763758 \tLoss: 0.995746215233953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 valid: 100%|█████████████████████████| 453/453 [01:41<00:00,  4.47it/s, acc=0.5, loss=0.945]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.0613147172969961\n",
      "* Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.87      0.76      8003\n",
      "           1       0.34      0.33      0.34      2113\n",
      "           2       0.35      0.19      0.24      2186\n",
      "           3       0.27      0.12      0.16      2168\n",
      "\n",
      "    accuracy                           0.57     14470\n",
      "   macro avg       0.41      0.37      0.37     14470\n",
      "weighted avg       0.51      0.57      0.53     14470\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 train: 100%|██████████████████████| 1809/1809 [20:14<00:00,  1.49it/s, acc=0.545, loss=1.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain ACC: 0.606472058897432 \tLoss: 0.984602918968865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 valid: 100%|████████████████████████| 453/453 [01:41<00:00,  4.47it/s, acc=0.667, loss=1.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.0746362504053852\n",
      "* Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.94      0.76      8003\n",
      "           1       0.49      0.10      0.17      2113\n",
      "           2       0.34      0.34      0.34      2186\n",
      "           3       0.00      0.00      0.00      2168\n",
      "\n",
      "    accuracy                           0.59     14470\n",
      "   macro avg       0.37      0.34      0.32     14470\n",
      "weighted avg       0.47      0.59      0.49     14470\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 train: 100%|█████████████████████| 1809/1809 [20:15<00:00,  1.49it/s, acc=0.455, loss=1.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain ACC: 0.6109132368460727 \tLoss: 0.9744605652992075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 valid: 100%|███████████████████████| 453/453 [01:41<00:00,  4.47it/s, acc=0.667, loss=1.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.0726215982805551\n",
      "* Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.85      0.75      8003\n",
      "           1       0.38      0.24      0.30      2113\n",
      "           2       0.31      0.42      0.35      2186\n",
      "           3       0.21      0.01      0.01      2168\n",
      "\n",
      "    accuracy                           0.57     14470\n",
      "   macro avg       0.39      0.38      0.35     14470\n",
      "weighted avg       0.51      0.57      0.52     14470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, AdamW, BertConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # 参数设置\n",
    "    model_path = r'model/bert-base-chinese/'\n",
    "    data_path = r'data/weibo_4/'\n",
    "    batch_size = 32\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    epochs = 10\n",
    "    learning_rate = 5e-6    #Learning Rate不宜太大\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    # 获取到dataset\n",
    "    train_dataset = SinaNewsDataset(data_path + 'train.csv', tokenizer)\n",
    "    valid_dataset = SinaNewsDataset(data_path + 'valid.csv', tokenizer)\n",
    "\n",
    "\n",
    "    # 生成Batch\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    # 读取BERT的配置文件\n",
    "    bert_config = BertConfig.from_pretrained(model_path)\n",
    "    num_labels = len(train_dataset.labels)\n",
    "\n",
    "    # 初始化模型\n",
    "    model = BertClassifier(bert_config, num_labels).to(device)\n",
    "\n",
    "    # 优化器\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    # 损失函数\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_f1 = 0\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        losses = 0      # 损失\n",
    "        accuracy = 0    # 准确率\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        train_bar = tqdm(train_dataloader, ncols=100)\n",
    "        for input_ids, token_type_ids, attention_mask, label_id in train_bar:\n",
    "            # 梯度清零\n",
    "            model.zero_grad()\n",
    "            train_bar.set_description('Epoch %i train' % epoch)\n",
    "\n",
    "            # 传入数据，调用model.forward()\n",
    "            output = model(\n",
    "                input_ids=input_ids.to(device), \n",
    "                attention_mask=attention_mask.to(device), \n",
    "                token_type_ids=token_type_ids.to(device), \n",
    "            )\n",
    "\n",
    "            # 计算loss\n",
    "            loss = criterion(output, label_id.to(device))\n",
    "            losses += loss.item()\n",
    "\n",
    "            pred_labels = torch.argmax(output, dim=1)   # 预测出的label\n",
    "            acc = torch.sum(pred_labels == label_id.to(device)).item() / len(pred_labels) #acc\n",
    "            accuracy += acc\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_bar.set_postfix(loss=loss.item(), acc=acc)\n",
    "\n",
    "        average_loss = losses / len(train_dataloader)\n",
    "        average_acc = accuracy / len(train_dataloader)\n",
    "\n",
    "        print('\\tTrain ACC:', average_acc, '\\tLoss:', average_loss)\n",
    "\n",
    "        # 验证\n",
    "        model.eval()\n",
    "        losses = 0      # 损失\n",
    "        pred_labels = []\n",
    "        true_labels = []\n",
    "        valid_bar = tqdm(valid_dataloader, ncols=100)\n",
    "        for input_ids, token_type_ids, attention_mask, label_id  in valid_bar:\n",
    "            valid_bar.set_description('Epoch %i valid' % epoch)\n",
    "\n",
    "            output = model(\n",
    "                input_ids=input_ids.to(device), \n",
    "                attention_mask=attention_mask.to(device), \n",
    "                token_type_ids=token_type_ids.to(device), \n",
    "            )\n",
    "            \n",
    "            loss = criterion(output, label_id.to(device))\n",
    "            losses += loss.item()\n",
    "\n",
    "            pred_label = torch.argmax(output, dim=1)   # 预测出的label\n",
    "            acc = torch.sum(pred_label == label_id.to(device)).item() / len(pred_label) #acc\n",
    "            valid_bar.set_postfix(loss=loss.item(), acc=acc)\n",
    "\n",
    "            pred_labels.extend(pred_label.cpu().numpy().tolist())\n",
    "            true_labels.extend(label_id.numpy().tolist())\n",
    "\n",
    "        average_loss = losses / len(valid_dataloader)\n",
    "        print('\\tLoss:', average_loss)\n",
    "        \n",
    "        # 分类报告\n",
    "        report = metrics.classification_report(true_labels, pred_labels, labels=valid_dataset.labels_id, target_names=valid_dataset.labels)\n",
    "        print('* Classification Report:')\n",
    "        print(report)\n",
    "\n",
    "        # f1 用来判断最优模型\n",
    "        f1 = metrics.f1_score(true_labels, pred_labels, labels=valid_dataset.labels_id, average='micro')\n",
    "        \n",
    "        if not os.path.exists('models'):\n",
    "            os.makedirs('models')\n",
    "        \n",
    "        # 判断并保存验证集上表现最好的模型\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save(model.state_dict(), 'models/best_model.pkl')\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
